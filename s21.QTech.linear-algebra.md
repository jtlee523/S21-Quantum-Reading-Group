# Linear Algebra (Physics-Focused)

## Tags
Matrices, Eigenvalues, Eigenvectors, Braket notation, Dirac Notation, Hermitian Operators

## Resources
[Barton Zwiebach's 8.05 OCW lecture notes](https://ocw.mit.edu/courses/physics/8-05-quantum-physics-ii-fall-2013/lecture-notes/)
- Specifically lecture notes for lectures 3-9

[Gilbert Strang's 18.06 OCW Lectures](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/)
- His book, *Linear Algebra and its applications*, or *Introduction to Linear Algebra*, are also great. 

This note doesn't do justice for all the math that Prof. Zwiebach covers...but I think it's enough to get started!

## Courses

While I generally recommend learning linear algebra on your own, you could take **MATH UN2010**, **APMA E3101**, **APMA E4001**, or **Honors Math** to learn some linear algebra!


## tldr

This note covers some basic linear algebra (using **Dirac notation** and a physics-focused perspective). This should be enough to get you up to speed to learn quantum computing specifically, but this is in no way exhaustive!

**Linear Algebra** is one of the major math pillars of physics - a strong foundation will take you very far!


# State Vectors and Vector Spaces

In **quantum mechanics**, wave functions can be interpreted as a **vector** in a complex vector space known as the **Hilbert Space**, $\mathcal{H}$. A state will be denoted by a **ket**, $| \psi \rangle$. We can think of the Hilbert space as containing all the possible wave vectors that are physically possible for our system. Each ket is just one possible wave vector.


# Axioms 

A **complex vector space** (and therefore for our Hilbert space) must satisfy the following axioms:

Let $|\psi_1 \rangle, |\psi_2 \rangle, |\psi_3 \rangle \in \mathcal{H}$ be three vectors in our Hilbert space.

- **Closure**: *Any linear combination of vectors within the vector space must also be in the vector space:* $\forall c_1, c_2 \in \mathcal{C}$ (complex numbers), $c_1|\psi_1\rangle + c_2|\psi_2\rangle \in \mathcal{H}$.  

- **Associativity and Commutativity of addition**: *How we group things we add and the order of which we add them doesn't matter:* $|\psi_1 \rangle + |\psi_2 \rangle = |\psi_2 \rangle + |\psi_1 \rangle$, and $|\psi_1 \rangle + (|\psi_2 \rangle + |\psi_3 \rangle) =|(\psi_1 \rangle + |\psi_2 \rangle) + |\psi_3 \rangle$.

- **Distributivity**: *Standard Multiplication and addition distributivity applied:* $c_1(|\psi_1 + |\psi_2\rangle) = c_1 |\psi_1 \rangle + c_1 |\psi_2\rangle$, and $(c_1 + c_2)|\psi_1 \rangle = c_1 |\psi_1\rangle + c_2|\psi_1\rangle$.

- **Inverse**: *There exists an vecor in our vector space that can add to zero with another vector:* $\forall |\psi\rangle \in \mathcal{H}, \exists (-|\psi\rangle)\in \mathcal{H}$ such that $|\psi\rangle + (-|\psi\rangle) = 0$.

- **Identity**: *There exists a vector that is basically an add by zero:* $\exists 0 \in \mathcal{H}$ such that $\forall |\psi\rangle \in \mathcal{H}, 0 + |\psi\rangle = |\psi\rangle$. 


*There are other axioms as well, but these are the important ones for us.


# Definitions 
Let $c_i \in \mathcal{C}$ and $|\psi_i \rangle \in \mathcal{H}$ ($i = 0, 1, 2, \dots$). A **linear combination** of the vectors $\{ |\psi_i \rangle \}$ can be written generally as:

$$ |\psi \rangle = c_0 |\psi_0 \rangle + c_1 |\psi_1 \rangle + \dots = \sum_i c_i|\psi_i\rangle$$

A set of vectors $\{ |\psi_i \rangle \}$ ($i = 0, 1, 2, \dots$) is called **linearly independent** if none of the vectors $|\psi_i\rangle$ can be written as a linear combination of the other vectors in the set. 

A set of vectors forms a **basis** if every $|\psi \rangle \in \mathcal{H}$ can be expressed as a linear combination of the set of vectors. 

The **dimension** of the vector space is given by the number of vectors in the basis. Our basis can be finite or infinite dimensional.


# Properties of State Vectors

For an $N$-dimensional vector space, we can express any wavector $|\psi\rangle$ as an $N$-dimensional column vector. *The representation is basis dependent.* For example, if we have a two-level system in the form $|\psi \rangle = c_1 |1\rangle + c_2|2\rangle$, we might express this state as:

$$ |\psi \rangle = \begin{pmatrix} c_1 \\ c_2 \end{pmatrix}$$

where we are in the basis of $|1\rangle$, $|2\rangle$. 

The complex conjugate of a ket is called a **bra**. 

$$ (|\psi \rangle)^\dagger = \langle \psi |$$

In the column vector notation, it is convenient to write the state as:

$$ \langle \psi |  = \begin{pmatrix} c_1^* & c_2^* \end{pmatrix}$$

Let $|\alpha \rangle = a_1 |1\rangle + a_2|2\rangle$, $|\beta \rangle = b_1 |1\rangle + b_2|2\rangle$. The **scalar product** between $\langle \beta |$ and $|\alpha\rangle$  is given as:

$$ \begin{aligned} \langle \beta | \alpha \rangle &= (b_1^* \langle 1| + b_2^*\langle 2|)(a_1 |1\rangle + a_2|2\rangle)  \\
&= a_1 b_1^* + a_2 b_2^*\end{aligned}$$

Since we assume that we work in an orthonormal basis, we have that $\langle i | j \rangle = \delta_{ij}$. This just means that for any $i$ and $j$, we get $\langle i | j \rangle=0$ unless $i=j$. If $i=j$, we get 1. 

We could also evaluate this using the column vector notation:
$$ \begin{aligned} \langle \beta | \alpha \rangle &=  \begin{pmatrix} b_1^* & b_2^* \end{pmatrix}\begin{pmatrix} a_1 \\ a_2 \end{pmatrix}\\
&= a_1 b_1^* + a_2 b_2^*\end{aligned}$$

The **norm** of a vector is defined as:

$$ \left| |\psi \rangle \right| = \sqrt{\langle \psi | \psi \rangle}$$

In quantum mechanics, our state vectors are often normalized. This means that $\langle \psi | \psi \rangle = 1$. 

Given an orthonormal basis $\{ |v_n \rangle\}$, we can express an identity operator as:

$$ \pmb{1} = \sum_n |v_n \rangle \langle v_n|$$

This property is known as **completeness**. 

# Operators

Let $|\psi \rangle \in \mathcal{H}$. A **linear operator** $\hat{A}$ will take $|\psi \rangle$ and take it to some other fector $|\psi ' \rangle = A|\psi\rangle \in \mathcal{H}$. 

When given a basis, we can express our operator as **matrix elements**. For example, consider a state $|\psi \rangle = \sum_n c_n|v_n\rangle$. Then, we have that:

$$ |\psi ' \rangle = \sum_n c_n \hat{A} |v_n\rangle$$

Because $|\psi' \rangle \in \mathcal{H}$, we can express it as a linear combination of our basis vectors, $|\psi '\rangle = \sum_m c_m' |v_m\rangle$. To solve for what $c_m'$ is, we can apply $\langle v_m |$ on the left side:

$$\begin{aligned} \langle v_m |\psi ' \rangle &= \sum_n c_n \langle v_m |\hat{A} |v_n\rangle \\
\langle v_m \sum_m c_m' |v_m\rangle &= \sum_n c_n \langle v_m |\hat{A} |v_n\rangle \\
 c_m &= \sum_n c_n A_{mn}\end{aligned} $$

The $A_{mn}$ are called matrix elements. If we express $\hat{A}$ as a matrix in the basis $\{ |v_n\rangle \}$, the $(m,n)$th entry in the matrix will be $A_{mn}$. We have that $(A^\dagger)_{mn} = A_{nm}^*$. 

The **projection operator** takes the form $|v_n \rangle \langle v_n|$ and gives us the vector component along the basis vector $|v_n \rangle \langle v_n|$. A more formal definition of the projection operator $\hat{P}$ is any operator that satisfies $\hat{P}^2 = \hat{P}$. 

A **unitary operator** is one that preserves scalar product and norm. If $\langle \psi |\psi \rangle = 1$, then $\langle \hat{U}\psi |\hat{U}\psi \rangle = 1$ for any unitary operator $\hat{U}$. Unitary operators also satisfy $\hat{U} \hat{U}^\dagger = \hat{U}^\dagger\hat{U} = \pmb{1}$. 

A **hermitian operator** is an operator such that $\hat{O} = \hat{O}^\dagger$. That is, the complex conjugate transpose of the operator (or **adjoint***) is equivalent to the operator itself. A hermitian operator will always have real eigenvalues and orthonormal eigenvectors, and thus form a basis. All observables in quantum mechanics are hermitian operators (but not all hermitian operators are observables). 

If we have that:

$$\hat{A} |\psi \rangle = \lambda | \psi \rangle$$

then we say that $|\psi\rangle$ is an **eigenvector** of $\hat{A}$ with **eigenvalue** $\lambda$. We can solve for the eigenvalues by solving $det(\hat{A} - \lambda \pmb{1}) = 0$.

# Problems 

1) Show that the eigenvalues of a hermitian operator are real.

2) Verify that for a sum of orthogonal projection operators $\hat{P} = \sum_i \hat{P}_i = \sum_i |i \rangle \langle i|$, we have that $\hat{P}^2 = \hat{P}$. (We assume that $\langle i| j \rangle = \delta_{ij}$.)

3) Show that for any projection operator $\hat{P}$ (satisfying $\hat{P}^2 = \hat{P}$) that the eigenvalues must be either 0 or 1.

